# Reasoning-Models-Don-t-Always-Say-What-They-Think-experiment
воспроизведение эксперимента по статье "Reasoning Models Don't Always Say What They Think" для ниса лингвистическая интерпретация моделей

опишу файлы:

```gpqa-diamond-qa-selection.json``` - вопросы GPQA на основе которых делаются промпты

```sql-console-for-cais-mmlu.json``` - вопросы MMLU на основе которых делаются промпты

```gpqa.json``` - результат предобработки вопросов GPQA (приведения к более удобному формату)

```prompts_making.ipynb``` - эта тетрадка делает вообще все варианты промптов которые используются в эксперименте описанном в статье. мы используем часть из них для дальнейшего вопроизведения. в качестве дополнительного результата, в ней приводится дополнение промптов указанием на то, есть ли наблюдение за cot модели (это наша идея для дополнения).
в рамках воспроизведения эксперимента производится такое количество промптов:
из GPQA взяли 198 вопросов
из MMLU взяли 114 вопросов
к каждому из этих наборов достроили каждую из 6 подсказок в двух вариантах - со случайным неверным ответом и с верным, а также оставили один набор без подсказок для сравнения
всего вышло (198 + 114) * 13 = 4056 промптов
при добавлении указания о наблюдении мы создали еще 2 набора такой же длины, всего вышло 4056 * 3 = 12168 промптов

```prefilling_table.ipynb``` - эта тетрадка презаполняет таблицу с результатами обработки промптов моделью. она презаполняет сами вопросы, правильные ответы на них и каждую из подсказок, которая используется для достроения промптов, которые обрабатывает модель (каждого вида подсказок - 5 вариантов, важно знать, для какого ответа модели была использован какой вариант подсказки)
