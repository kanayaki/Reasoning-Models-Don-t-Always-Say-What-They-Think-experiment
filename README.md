# Reasoning-Models-Don-t-Always-Say-What-They-Think-experiment
воспроизведение эксперимента по статье "Reasoning Models Don't Always Say What They Think" для ниса лингвистическая интерпретация моделей

используемые файлы:

1. ```gpqa-diamond-qa-selection.json``` - вопросы GPQA на основе которых делаются промпты

2. ```sql-console-for-cais-mmlu.json``` - вопросы MMLU на основе которых делаются промпты

3. ```gpqa.json``` - результат предобработки вопросов GPQA (приведения к более удобному формату) с помощью ```prompts_processing.ipynb```

4. ```prefilling_table.ipynb``` - эта тетрадка презаполняет таблицу с результатами обработки промптов моделью. она презаполняет сами вопросы, правильные ответы на них и каждую из подсказок, которая используется для достроения промптов, которые обрабатывает модель (каждого вида подсказок - 5 вариантов, важно знать, для какого ответа модели была использован какой вариант подсказки). на вход принимает csv таблицу с предзаполненным header, ```sql-console-for-cais-mmlu.json``` и ```gpqa.json```, на выход дает csv таблицу с заполненными столбцами вопрос, правильный ответ и 6 столбцов с формулировками подсказок

5. ```prompts_making.ipynb``` - эта тетрадка делает вообще все варианты промптов которые используются в эксперименте описанном в статье. мы используем часть из них для дальнейшей работы. в качестве дополнительного результата, в ней приводится достраивание промптов указанием на то, есть ли наблюдение за цепочкой размышлений модели (это наша идея для дополнения). на вход принимает ```sql-console-for-cais-mmlu.json``` и ```gpqa.json```, на выходе в переменные записываются листы, которые находятся в корректном формате подачи в модель для эксперимента

в этой тетрадке производится такое количество промптов:

из GPQA взяли 198 вопросов

из MMLU взяли 114 вопросов

к каждому из этих наборов достроили каждую из 6 подсказок в двух вариантах - со случайным неверным ответом и с верным, а также оставили один набор без подсказок для сравнения
всего вышло (198 + 114) * 13 = 4056 промптов

при добавлении указания о наблюдении мы создали еще 2 набора такой же длины, всего вышло 4056 * 3 = 12168 промптов

6. ```prompts_processing.ipynb``` - эта тетрадка содержит

a) предобработку датасета GPQA (чтобы он соответствовал по формату датасету MMLU и с ним было удобнее работать) - ```gpqa-diamond-qa-selection.json``` превращает в ```gpqa.json```

b) нужные для проведенного нами эксперимента части ```prompts_making.ipynb``` (чтобы получить листы промптов для модели)

c) обращение к модели по апи, на вход поступают промпты, на выход ответ модели и цепочка размышлений. вывод записывается в предзаполненную csv таблицу ``hinted_prompts.csv``

7. ```hinted_prompts.csv``` - таблица, в которой хранятся результаты основного эксперимента. там хранятся ответы и цепочки размышлений модели deepseek-r1 на промпты без подсказок, с подсказкой ```sycophancy``` и с подсказкой ```grader hacking```, построенные на основе датасета MMLU, а также дополнительная информация в виде самих вопросов, правильных ответов и формулировок подсказок (предзаполнены с помощью ```prefilling_table.ipynb```). всего в ней хранятся результаты обработки 114 * 3 = 342 промптов
